# _*_coding=UTF-8_*_

import time
import lightgbm as lgb
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from keras_preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical, plot_model
from keras.models import Model
from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, LSTM, AveragePooling1D, Maximum
from keras.layers import Dense, Embedding, Input, SpatialDropout1D, Dropout, concatenate
from keras.callbacks import ModelCheckpoint, EarlyStopping


def preprocess(trainset, testset):
    original_trainset_df = pd.read_csv(trainset, encoding='utf-8')
    train_api_sequences, train_labels = [], []
    for file, file_df in original_trainset_df.groupby('file_id'):
        api_sequence = []
        for tid, tid_df in file_df.groupby('tid'):
            api_sequence += tid_df.sort_values(['index'], ascending=True)['api'].tolist()
        train_api_sequences.append(' '.join(api_sequence))
        train_labels.append(int(file_df.head(1)['label']))
    with open('tmp/trainset.txt', 'w') as wf:
        for i in range(len(train_labels)):
            wf.write(str(train_labels[i]) + ' ' + train_api_sequences[i] + '\n')

    original_test_df = pd.read_csv(testset, encoding='utf-8')
    test_api_sequences = []
    for file, file_df in original_test_df.groupby('file_id'):
        api_sequence = []
        for tid, tid_df in file_df.groupby('tid'):
            api_sequence += tid_df.sort_values(['index'], ascending=True)['api'].tolist()
        test_api_sequences.append(' '.join(api_sequence))
    with open('tmp/testset.txt', 'w') as wf:
        for i in range(len(test_api_sequences)):
            wf.write(test_api_sequences[i] + '\n')

    print('训练集大小: ' + str(len(train_api_sequences)))
    print('测试集大小: ' + str(len(test_api_sequences)))

    return train_api_sequences, train_labels, test_api_sequences


def tfidf_lgb_train(train_api_sequences, train_labels, test_api_sequences):
    train_labels = np.array(train_labels)
    vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_df=0.9)
    train_tfidf_features = vectorizer.fit_transform(train_api_sequences).toarray()
    test_tfidf_features = vectorizer.transform(test_api_sequences).toarray()
    print('训练集tfidf特征shape: ' + str(train_tfidf_features.shape))
    print('测试集tfidf特征shape: ' + str(test_tfidf_features.shape))

    tfidf_lgb_train_result = np.zeros(shape=(len(train_tfidf_features), 8))
    tfidf_lgb_test_result = np.zeros(shape=(len(test_tfidf_features), 8))
    skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)
    for fold_, (train_, val_) in enumerate(skf.split(train_tfidf_features, train_labels)):
        X_train, y_train = train_tfidf_features[train_], train_labels[train_]
        X_val, y_val = train_tfidf_features[val_], train_labels[val_]
        lgb_train = lgb.Dataset(X_train, y_train)
        lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)

        params = {'task': 'train',
                  'boosting_type': 'gbdt',  # 设置提升类型
                  'objective': 'multiclass',  # 目标函数
                  'metric': 'multi_logloss',  # 评估函数
                  'num_leaves': 31,  # 叶子节点数
                  'num_class': 8,   # 类别数
                  'learning_rate': 0.05,  # 学习速率
                  'feature_fraction': 0.9,  # 建树的特征选择比例
                  'bagging_fraction': 0.8,  # 建树的样本采样比例
                  'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging
                  'verbose': -1}  # <0 显示致命的, =0 显示错误 (警告), >0 显示信息
        lgbm = lgb.train(params, lgb_train,
                         num_boost_round=300,
                         valid_sets=lgb_val,
                         early_stopping_rounds=50)
        lgbm.save_model('model/tfidf_lgb_model_{}.pickle'.format(str(fold_)))

        pred_val = lgbm.predict(X_val)
        pred_test = lgbm.predict(test_tfidf_features)
        tfidf_lgb_train_result[val_] = pred_val
        tfidf_lgb_test_result += pred_test

    tfidf_lgb_test_result = tfidf_lgb_test_result / 5.0
    tfidf_lgb_result = [tfidf_lgb_train_result, tfidf_lgb_test_result]

    return tfidf_lgb_result


def text_cnn_model(maxlen, input_dim):
    input = Input(shape=(maxlen,), dtype='float64')
    embedding = Embedding(input_dim, 256, input_length=maxlen)(input)
    embedding = SpatialDropout1D(0.25)(embedding)  # 随机对特定维度全部置零
    convs = []
    for kernel_size in [3, 4, 5]:
        for dilated_rate in [1, 2, 3, 4]:
            conv1d = Conv1D(filters=64,
                            kernel_size=kernel_size,
                            activation='relu',
                            dilation_rate=dilated_rate)(embedding)
            convs.append(GlobalMaxPooling1D()(conv1d))
    convs = concatenate(convs)

    drop1 = Dropout(0.5)(convs)
    dense = Dense(256, activation='relu')(drop1)
    drop2 = Dropout(0.25)(dense)
    output = Dense(8, activation='softmax')(drop2)

    model = Model(inputs=input, outputs=output)
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    plot_model(model, to_file='tmp/text_cnn_model.png', show_shapes=True)

    return model


def text_cnn_train(train_api_sequences, train_labels, test_api_sequences):
    maxlen = 6000
    tokenizer = Tokenizer(num_words=None,
                          filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~',
                          split=' ',
                          char_level=False,
                          oov_token=None)
    tokenizer.fit_on_texts(train_api_sequences)
    tokenizer.fit_on_texts(test_api_sequences)
    api_type_ct = len(tokenizer.index_word)
    print('Text_cnn_train, Number of api types: ' + str(api_type_ct))
    train_word_ids = tokenizer.texts_to_sequences(train_api_sequences)
    train_padded_seqs = pad_sequences(train_word_ids, maxlen=maxlen)
    test_word_ids = tokenizer.texts_to_sequences(test_api_sequences)
    test_padded_seqs = pad_sequences(test_word_ids, maxlen=maxlen)
    train_labels_ctg = to_categorical(train_labels, num_classes=8)

    text_cnn_train_result = np.zeros(shape=(len(train_padded_seqs), 8))
    text_cnn_test_result = np.zeros(shape=(len(test_padded_seqs), 8))
    skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)
    model = text_cnn_model(maxlen, api_type_ct + 1)
    for fold_, (train_, val_) in enumerate(skf.split(train_padded_seqs, train_labels)):
        X_train, y_train = train_padded_seqs[train_], train_labels_ctg[train_]
        X_val, y_val = train_padded_seqs[val_], train_labels_ctg[val_]

        earlystoper = EarlyStopping(monitor='val_loss',
                                    min_delta=0,
                                    patience=3,
                                    verbose=0,
                                    mode='min',
                                    baseline=None,
                                    restore_best_weights=False)
        checkpointer = ModelCheckpoint(filepath='model/text_cnn_model_{}.pickle'.format(str(fold_)),
                                       verbose=0,
                                       save_best_only=True)
        history = model.fit(X_train, y_train,
                            validation_data=(X_val, y_val),
                            shuffle=True,
                            batch_size=128,
                            epochs=100,
                            verbose=0,
                            callbacks=[earlystoper, checkpointer]).history

        pred_val = model.predict(X_val)
        pred_test = model.predict(test_padded_seqs)
        text_cnn_train_result[val_] = pred_val
        text_cnn_test_result += pred_test

    text_cnn_test_result = text_cnn_test_result / 5.0
    text_cnn_result = [text_cnn_train_result, text_cnn_test_result]

    return text_cnn_result


def cnn_lstm_model(maxlen, input_dim):
    input = Input(shape=(maxlen,), dtype='float64')
    embedding = Embedding(input_dim, 256, input_length=maxlen)(input)

    conv1_1 = Conv1D(filters=64,  kernel_size=3, padding='same', activation='relu')(embedding)
    conv1_2 = Conv1D(filters=64, kernel_size=3, activation='relu')(conv1_1)
    conv1 = MaxPool1D(pool_size=2)(conv1_2)

    conv2_1 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(conv1)
    conv2_2 = Conv1D(filters=64, kernel_size=3, activation='relu')(conv2_1)
    conv2 = MaxPool1D(pool_size=2)(conv2_2)

    conv3_1 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(conv2)
    conv3_2 = Conv1D(filters=64, kernel_size=3, activation='relu')(conv3_1)
    conv3 = MaxPool1D(pool_size=2)(conv3_2)

    lstm = LSTM(256)(conv3)
    dense = Dense(256, activation='relu')(lstm)
    output = Dense(8, activation='softmax')(dense)
    model = Model(inputs=input, outputs=output)
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    plot_model(model, to_file='tmp/cnn_lstm_model.png', show_shapes=True)

    return model


def cnn_lstm_train(train_api_sequences, train_labels, test_api_sequences):
    maxlen = 6000
    tokenizer = Tokenizer(num_words=None,
                          filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~',
                          split=' ',
                          char_level=False,
                          oov_token=None)
    tokenizer.fit_on_texts(train_api_sequences)
    tokenizer.fit_on_texts(test_api_sequences)
    api_type_ct = len(tokenizer.index_word)
    print('Cnn_lstm_train, Number of api types: ' + str(api_type_ct))
    train_word_ids = tokenizer.texts_to_sequences(train_api_sequences)
    train_padded_seqs = pad_sequences(train_word_ids, maxlen=maxlen)
    test_word_ids = tokenizer.texts_to_sequences(test_api_sequences)
    test_padded_seqs = pad_sequences(test_word_ids, maxlen=maxlen)
    train_labels_ctg = to_categorical(train_labels, num_classes=8)

    cnn_lstm_train_result = np.zeros(shape=(len(train_padded_seqs), 8))
    cnn_lstm_test_result = np.zeros(shape=(len(test_padded_seqs), 8))
    skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)
    model = cnn_lstm_model(maxlen, api_type_ct + 1)
    for fold_, (train_, val_) in enumerate(skf.split(train_padded_seqs, train_labels)):
        X_train, y_train = train_padded_seqs[train_], train_labels_ctg[train_]
        X_val, y_val = train_padded_seqs[val_], train_labels_ctg[val_]

        earlystoper = EarlyStopping(monitor='val_loss',
                                    min_delta=0,
                                    patience=3,
                                    verbose=0,
                                    mode='min',
                                    baseline=None,
                                    restore_best_weights=False)
        checkpointer = ModelCheckpoint(filepath='model/cnn_lstm_model_{}.pickle'.format(str(fold_)),
                                       verbose=0,
                                       save_best_only=True)
        history = model.fit(X_train, y_train,
                            validation_data=(X_val, y_val),
                            shuffle=True,
                            batch_size=128,
                            epochs=100,
                            verbose=0,
                            callbacks=[earlystoper, checkpointer]).history

        pred_val = model.predict(X_val)
        pred_test = model.predict(test_padded_seqs)
        cnn_lstm_train_result[val_] = pred_val
        cnn_lstm_test_result += pred_test

    cnn_lstm_test_result = cnn_lstm_test_result / 5.0
    cnn_lstm_result = [cnn_lstm_train_result, cnn_lstm_test_result]

    return cnn_lstm_result


def mulit_lstm_model(maxlen, input_dim):
    input = Input(shape=(maxlen,), dtype='float64')
    embedding = Embedding(input_dim, 256, input_length=maxlen)(input)
    convs1, convs2, convs3 = [], [], []
    for kernel_size in [3, 4, 5]:
        conv1 = Conv1D(filters=64,
                       kernel_size=kernel_size,
                       activation='relu',
                       padding='same')(embedding)
        convs1.append(AveragePooling1D()(conv1))
    for (kernel_size, conv1) in zip([3, 4, 5], convs1):
        conv2 = Conv1D(filters=64,
                       kernel_size=kernel_size,
                       activation='relu',
                       padding='same')(conv1)
        convs2.append(AveragePooling1D()(conv2))
    for (kernel_size, conv2) in zip([3, 4, 5], convs2):
        conv3 = Conv1D(filters=64,
                       kernel_size=kernel_size,
                       activation='relu',
                       padding='same')(conv2)
        convs3.append(AveragePooling1D()(conv3))

    convs_max = Maximum()(convs3)
    lstm = LSTM(512)(convs_max)
    output = Dense(8, activation='softmax')(lstm)
    model = Model(inputs=input, outputs=output)
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    plot_model(model, to_file='tmp/multi_lstm_model.png', show_shapes=True)

    return model


def multi_lstm_train(train_api_sequences, train_labels, test_api_sequences):
    maxlen = 6000
    tokenizer = Tokenizer(num_words=None,
                          filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~',
                          split=' ',
                          char_level=False,
                          oov_token=None)
    tokenizer.fit_on_texts(train_api_sequences)
    tokenizer.fit_on_texts(test_api_sequences)
    api_type_ct = len(tokenizer.index_word)
    print('Multi_lstm_train, Number of api types: ' + str(api_type_ct))
    train_word_ids = tokenizer.texts_to_sequences(train_api_sequences)
    train_padded_seqs = pad_sequences(train_word_ids, maxlen=maxlen)
    test_word_ids = tokenizer.texts_to_sequences(test_api_sequences)
    test_padded_seqs = pad_sequences(test_word_ids, maxlen=maxlen)
    train_labels_ctg = to_categorical(train_labels, num_classes=8)

    multi_lstm_train_result = np.zeros(shape=(len(train_padded_seqs), 8))
    multi_lstm_test_result = np.zeros(shape=(len(test_padded_seqs), 8))
    skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)
    model = mulit_lstm_model(maxlen, api_type_ct + 1)
    for fold_, (train_, val_) in enumerate(skf.split(train_padded_seqs, train_labels)):
        X_train, y_train = train_padded_seqs[train_], train_labels_ctg[train_]
        X_val, y_val = train_padded_seqs[val_], train_labels_ctg[val_]

        earlystoper = EarlyStopping(monitor='val_loss',
                                    min_delta=0,
                                    patience=3,
                                    verbose=0,
                                    mode='min',
                                    baseline=None,
                                    restore_best_weights=False)
        checkpointer = ModelCheckpoint(filepath='model/multi_lstm_model_{}.pickle'.format(str(fold_)),
                                       verbose=0,
                                       save_best_only=True)
        history = model.fit(X_train, y_train,
                            validation_data=(X_val, y_val),
                            shuffle=True,
                            batch_size=128,
                            epochs=100,
                            verbose=0,
                            callbacks=[earlystoper, checkpointer]).history

        pred_val = model.predict(X_val)
        pred_test = model.predict(test_padded_seqs)
        multi_lstm_train_result[val_] = pred_val
        multi_lstm_test_result += pred_test

    multi_lstm_test_result = multi_lstm_test_result / 5.0
    multi_lstm_result = [multi_lstm_train_result, multi_lstm_test_result]

    return multi_lstm_result


def stack(tfidf_lgb_result, text_cnn_result, cnn_lstm_result, multi_lstm_result, train_labels, test_api_sequences):
    train_labels = np.array(train_labels)
    models_train_result = np.hstack([tfidf_lgb_result[0], text_cnn_result[0], cnn_lstm_result[0], multi_lstm_result[0]])
    print('训练集shape: ' + str(models_train_result.shape))
    models_test_result = np.hstack([tfidf_lgb_result[1], text_cnn_result[1], cnn_lstm_result[1], multi_lstm_result[1]])
    print('测试集shape: ' + str(models_test_result.shape))
    test_result = np.zeros(shape=(len(test_api_sequences), 8))
    skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)
    for fold_, (train_, val_) in enumerate(skf.split(models_train_result, train_labels)):
        X_train, y_train = models_train_result[train_], train_labels[train_]
        X_val, y_val = models_train_result[val_], train_labels[val_]
        lgb_train = lgb.Dataset(X_train, y_train)
        lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)
        params = {'task': 'train',
                  'boosting_type': 'gbdt',
                  'objective': 'multiclass',
                  'metric': 'multi_logloss',
                  'num_leaves': 31,
                  'num_class': 8,
                  'learning_rate': 0.05,
                  'feature_fraction': 0.9,
                  'bagging_fraction': 0.8,
                  'bagging_freq': 5,
                  'verbose': -1}
        lgbm = lgb.train(params, lgb_train, num_boost_round=300, valid_sets=lgb_val, early_stopping_rounds=50)
        pred_test = lgbm.predict(models_test_result)
        test_result += pred_test

    test_result = test_result / 5.0

    return test_result


def write_file(result):
    out = []
    for i in range(len(result)):
        tmp = []
        tmp.append(i+1)
        tmp = tmp + result[i].tolist()
        out.append(tmp)

    out_csv = pd.DataFrame(columns=["file_id", "prob0", "prob1", "prob2", "prob3", "prob4", "prob5", "prob6", "prob7"], data=out)
    out_csv.to_csv('result/detect_result_{}.csv'.format(
        str(time.strftime("%Y-%m-%d-%H-%M", time.localtime()))), index=False)


if __name__ == '__main__':
    train_api_sequences, train_labels, test_api_sequences = preprocess('dataset/tmp_train.csv', 'dataset/tmp_test.csv')
    tfidf_lgb_result = tfidf_lgb_train(train_api_sequences, train_labels, test_api_sequences)
    text_cnn_result = text_cnn_train(train_api_sequences, train_labels, test_api_sequences)
    cnn_lstm_result = cnn_lstm_train(train_api_sequences, train_labels, test_api_sequences)
    multi_lstm_result = multi_lstm_train(train_api_sequences, train_labels, test_api_sequences)
    result = stack(tfidf_lgb_result, text_cnn_result, cnn_lstm_result, multi_lstm_result, train_labels, test_api_sequences)
    write_file(result)

